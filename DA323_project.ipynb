{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe8b885",
   "metadata": {},
   "source": [
    "# DA323 Project- paper summary and explanation\n",
    "## Zero Shot Text to Image Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c47edf9",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- About the Paper  \n",
    "- Motivation  \n",
    "- Previous Approaches  \n",
    "- Method  \n",
    "- Datasets  \n",
    "- Significance and Results  \n",
    "- Future Impact  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288965a",
   "metadata": {},
   "source": [
    "# About the Paper\n",
    "\n",
    "**Paper Title:**  \n",
    "*Zero-Shot Text-to-Image Generation*  \n",
    "\n",
    "**Authors:**  \n",
    "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,  \n",
    "Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever (OpenAI)  \n",
    "\n",
    "**Published:**  \n",
    "arXiv preprint: [https://arxiv.org/abs/2102.12092](https://arxiv.org/abs/2102.12092)  \n",
    "Date: February 2021  \n",
    "\n",
    "**Core Contribution:**  \n",
    "A scalable, autoregressive transformer-based model that treats text and image tokens as a single sequence.  \n",
    "Capable of generating realistic images from text prompts **without any task-specific fine-tuning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9d4fe",
   "metadata": {},
   "source": [
    "## Motivation for this Paper\n",
    "\n",
    "Earlier models like **GANs** and **VAEs** struggle with generalization — that is, they perform poorly on unseen data.\n",
    "\n",
    "Major issues with GANs include:\n",
    "- **Mode collapse**  \n",
    "  The GAN ends up producing data very similar to only a few types (the “modes”), which adversely affects the variety of obtained multi-modal data.\n",
    "  \n",
    "- **Nonconvergence and instability**  \n",
    "  This can arise due to:\n",
    "  - Inappropriate design of network architecture  \n",
    "  - Poor choice of objective function  \n",
    "  - Suboptimal optimization algorithms\n",
    "\n",
    "Training instability is a critical problem. For example, if the discriminator can easily distinguish between fake and real images, its gradient vanishes —\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ebdf7",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/mode_collapse.png\" alt=\"Mode collapse\" width=\"600\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e33ad",
   "metadata": {},
   "source": [
    "\n",
    "Traditionally, **text-to-image synthesis** has been approached by improving modeling assumptions for training on a fixed dataset.\n",
    "\n",
    "These approaches often rely on:\n",
    "- Complex architectures  \n",
    "- Auxiliary losses  \n",
    "- Object part labels  \n",
    "- Dataset-specific design choices  \n",
    "\n",
    "Such methods can **hurt generalization**, making them less effective on unseen prompts or domains.\n",
    "\n",
    "---\n",
    "\n",
    "To counter these limitations, the authors propose a **simple and scalable approach**:\n",
    "- Use an **autoregressive transformer**  \n",
    "- Model **text and image tokens as a single stream of data**  \n",
    "\n",
    "With enough data, this method proves **competitive with task-specific models** when evaluated in a **zero-shot** setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81f4ad",
   "metadata": {},
   "source": [
    "## Generating Images from Captions with Attention\n",
    "\n",
    "In **Mansimov et al., 2015**, the authors extended the **Deep Recurrent Attention Writer (DRAW)** technique to train a model that:\n",
    "\n",
    "- Iteratively draws patches on a canvas  \n",
    "- Attends to relevant words in the text description at each step  \n",
    "\n",
    "### Key Mechanism:\n",
    "- The current hidden state for image generation is computed using:\n",
    "  - The **previous hidden state**, and  \n",
    "  - An **alignment score** between:\n",
    "    - The hidden states from the text  \n",
    "    - The previous image hidden state  \n",
    "\n",
    "- This current hidden state is then used to **generate the image at the current iteration**\n",
    "\n",
    "This was one of the **first deep learning-based approaches** for image synthesis from natural language descriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c18c27",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/variational_auto_encoder.png\" alt=\"dVAE Diagram\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe65473",
   "metadata": {},
   "source": [
    "### Figure: Conventional variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7add3c",
   "metadata": {},
   "source": [
    "## StackGAN++ (Xu et al., 2018)\n",
    "\n",
    "**StackGAN-v1** uses a **2-stage Generative Adversarial Network** for text-to-image synthesis:\n",
    "\n",
    "- **Stage I**:\n",
    "  - Sketches primitive features like **color** and **shape** based on the text description.\n",
    "  - Produces **low-resolution images**.\n",
    "\n",
    "- **Stage II**:\n",
    "  - Takes Stage I output **and** the original text as input.\n",
    "  - Generates **high-resolution images** by refining the initial sketch.\n",
    "\n",
    "---\n",
    "\n",
    "An extended approach (**StackGAN++**) uses:\n",
    "- **Multiple generators and discriminators** arranged in a **tree structure**.\n",
    "- Images at **multiple scales** for the same scene are generated from **different branches** of the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fcc655",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/stackgan-v1.png\" alt=\"dVAE Diagram\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdaaff0",
   "metadata": {},
   "source": [
    "### Method described in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c00ec8",
   "metadata": {},
   "source": [
    "## Key Innovation and Method\n",
    "\n",
    "The goal of this paper is to train a **transformer** that **autoregressively models both images and text as a single data stream**.\n",
    "\n",
    "However, directly using pixel values requires **too much memory**, especially for high-resolution images.  \n",
    "To overcome this, the authors **compress images into tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### Main Components of the Architecture:\n",
    "\n",
    "- **Discrete Variational Autoencoder (dVAE):**  \n",
    "  Compresses each **256×256 RGB image** into a **32×32 grid of image tokens**.\n",
    "\n",
    "- Each token can take on **8192 possible values**, i.e., from a discrete vocabulary of size 8192.\n",
    "\n",
    "- This reduces the **context length for the transformer by a factor of 192**,  \n",
    "  with **minimal degradation in visual quality** (as demonstrated in the next section).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b55c74",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/dVAE1.png\" alt=\"dVAE Diagram\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432da45",
   "metadata": {},
   "source": [
    "## The Second Stage of the Model\n",
    "\n",
    "- Up to **256 BPE-encoded text tokens** are concatenated with the **32 × 32 = 1024 image tokens**.\n",
    "- The **autoregressive transformer** is trained to model the **joint distribution over both text and image tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective Function and Interpretation:\n",
    "\n",
    "The model aims to learn the joint probability of:\n",
    "- **Text prompt** $y$\n",
    "- **Image** $x$\n",
    "- **Latent image tokens** $z$ (from the discrete VAE)\n",
    "\n",
    "This joint distribution is modeled as:\n",
    "\n",
    "**$p_{\\theta, \\psi}(x, y, z) = p_{\\theta}(x \\mid y, z) \\cdot p_{\\psi}(y, z)$**\n",
    "\n",
    "Where:\n",
    "- $p_{\\theta}(x \\mid y, z)$: Conditional image generation given text and image tokens  \n",
    "- $p_{\\psi}(y, z)$: Prior over text and image tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec223dca",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "Since we cannot directly optimize the joint probability $p_{\\theta, \\psi}(x, y)$,  \n",
    "we instead **maximize the Evidence Lower Bound (ELBO):**\n",
    "\n",
    "**ELBO:**\n",
    "\n",
    "$\\ln p_{\\theta, \\psi}(x, y) \\geq \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)} \\left[ \\ln p_{\\theta}(x \\mid y, z) \\right] - \\beta \\, D_{\\mathrm{KL}} \\left( q_{\\phi}(y, z \\mid x) \\,\\|\\, p_{\\psi}(y, z) \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of Terms:\n",
    "\n",
    "- $x$ — Input RGB image  \n",
    "- $y$ — Text caption  \n",
    "- $z$ — Discrete latent tokens from the dVAE  \n",
    "- $q_{\\phi}(z \\mid x)$ — Distribution over image tokens generated by the dVAE encoder  \n",
    "- $p_{\\theta}(x \\mid y, z)$ — Distribution over RGB images generated by the dVAE decoder  \n",
    "- $p_{\\psi}(y, z)$ — Prior modeled by the transformer over text and image tokens  \n",
    "- $\\beta$ — KL divergence weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab764cc",
   "metadata": {},
   "source": [
    "## Interpretation of the Objective Function\n",
    "\n",
    "Optimizing the term $\\ln p_{\\theta}(x \\mid y, z)$ helps improve the **reconstruction quality** of the dVAE.  \n",
    "This ensures that the **image token compression retains as much information** from the original image as possible.\n",
    "\n",
    "---\n",
    "\n",
    "The KL divergence term:\n",
    "\n",
    "$\n",
    "D_{\\mathrm{KL}} \\left( q_{\\phi}(y, z \\mid x) \\,\\|\\, p_{\\psi}(y, z) \\right)\n",
    "$\n",
    "\n",
    "acts as a **regularizer**, similar to its role in standard autoencoders:\n",
    "\n",
    "- It **prevents the dVAE encoder from encoding gibberish**\n",
    "- It discourages the encoder from **memorizing input images**\n",
    "- It encourages the encoder to **learn shared structure** across different images in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f00164",
   "metadata": {},
   "source": [
    "## Architecture in Brief: dVAE\n",
    "\n",
    "The **dVAE encoder and decoder** are convolutional **ResNets** (He et al., 2016), inspired by the bottleneck-style residual blocks introduced in earlier convolutional networks (LeCun et al., 1998).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Architecture Details:\n",
    "\n",
    "- Both encoder and decoder use primarily **3 × 3 convolutions**\n",
    "- **1 × 1 convolutions** are used in **skip connections** where the number of feature maps changes\n",
    "- The **first convolution of the encoder** is **7 × 7**\n",
    "- The **final encoder layer** is a **1 × 1 convolution** that produces a **32 × 32 × 8192** output (used as logits for the image token categorical distribution)\n",
    "- The **first and last layers of the decoder** are also **1 × 1 convolutions**\n",
    "\n",
    "---\n",
    "\n",
    "### Downsampling & Upsampling:\n",
    "\n",
    "- The **encoder** uses **max-pooling** to downsample feature maps  \n",
    "  *(empirically found to yield better ELBO than average pooling)*\n",
    "- The **decoder** uses **nearest-neighbor upsampling**\n",
    "\n",
    "---\n",
    "\n",
    "### Optimization:\n",
    "\n",
    "- Parameters are updated using the **AdamW optimizer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79842885",
   "metadata": {},
   "source": [
    "## Architecture in Brief: Transformer\n",
    "\n",
    "The authors present a **decoder-only transformer** designed to model the **joint distribution of text and image tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Overview:\n",
    "\n",
    "- A **12-billion parameter** transformer  \n",
    "- Processes sequences of:\n",
    "  - Up to **256 BPE-encoded text tokens**\n",
    "  - Followed by **1,024 image tokens** from the dVAE\n",
    "\n",
    "---\n",
    "\n",
    "### Attention Mechanism:\n",
    "\n",
    "- Uses **causal self-attention**, so each image token can attend to:\n",
    "  - All **preceding text tokens**\n",
    "  - All **preceding image tokens**\n",
    "\n",
    "This enables the generation of **coherent images conditioned on textual descriptions**.\n",
    "\n",
    "---\n",
    "\n",
    "### Modality-Specific Attention Masks:\n",
    "\n",
    "- **Text-to-text:** Standard **causal masks**\n",
    "- **Image-to-image:**  \n",
    "  Uses specialized masks like:\n",
    "  - **Row masks**\n",
    "  - **Column masks**\n",
    "  - **Convolutional masks**\n",
    "\n",
    "---\n",
    "\n",
    "### Key Property:\n",
    "\n",
    "This design allows the transformer to perform **zero-shot image generation** from text prompts —  **no task-specific fine-tuning** is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dad8a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To scale the model to **12 billion parameters**, the authors collected a much larger dataset of  \n",
    "**250 million image–text pairs from the internet**, comparable in scale to **JFT-300M**.\n",
    "\n",
    "- While **MS-COCO** was not explicitly included, some validation images (not captions) were present due to **overlap with YFCC100M**, one of the sources used.\n",
    "\n",
    "---\n",
    "\n",
    "### Filtering Techniques Used to Ensure Quality:\n",
    "\n",
    "- Captions that were **too short** or **not in English** (detected using `cld3`) were removed  \n",
    "- Captions with **generic boilerplate** text (e.g., \"photographed on \\<date\\>\") were discarded  \n",
    "- Images with **extreme aspect ratios** (outside the range [1/2, 2]) were excluded  \n",
    "  - This helps avoid cropping out important visual content during training\n",
    "\n",
    "---\n",
    "\n",
    "These preprocessing steps ensured that the resulting dataset was both **high-quality** and **diverse**, supporting effective training of the large-scale transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef058f30",
   "metadata": {},
   "source": [
    "## Significance and Results\n",
    "\n",
    "The dataset used in this paper — **250 million image–text pairs** — is significantly larger than previously used datasets such as **MS-COCO** and **CUB-200**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "> **More data + larger model = better generalization**\n",
    "\n",
    "---\n",
    "\n",
    "### Zero-Shot Evaluation:\n",
    "\n",
    "The model was evaluated in a **zero-shot setting** against three prior state-of-the-art approaches:\n",
    "\n",
    "- **AttnGAN** (Xu et al., 2018)  \n",
    "- **DMGAN** (Zhu et al., 2019)  \n",
    "- **DF-GAN** (Tao et al., 2020)  \n",
    "  - Reports the best **Inception Score (IS)** and **Fréchet Inception Distance (FID)** on MS-COCO  \n",
    "  - Metrics based on:  \n",
    "    - Inception Score: *Salimans et al., 2016*  \n",
    "    - FID: *Heusel et al., 2017*\n",
    "\n",
    "---\n",
    "\n",
    "### Human Evaluation:\n",
    "\n",
    "A human preference study, similar to **Koh et al. (2021)**, was conducted to compare the model's performance to DF-GAN.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb60682",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/results.png\" alt=\"Results\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae5bb3",
   "metadata": {},
   "source": [
    "### Results on CUB, a specialized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c789a0e",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/cub_results.png\" alt=\"CUB results\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608fb492",
   "metadata": {},
   "source": [
    "## Significance and Results \n",
    "\n",
    "The model performs **significantly worse on the CUB dataset**, with nearly a **40-point gap in FID** compared to the leading prior approach.\n",
    "\n",
    "- Although there was a **12% image overlap** found in the CUB dataset,  \n",
    "  the authors observed **no significant difference in results** after removing these overlapping images.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Observation:\n",
    "\n",
    "- The **zero-shot approach** tends to struggle on **specialized distributions** like CUB.\n",
    "- This suggests that **general-purpose models may underperform** on domain-specific datasets without adaptation.\n",
    "\n",
    "---\n",
    "\n",
    "### Future Direction:\n",
    "\n",
    "- The authors identify **fine-tuning** as a **promising path for improving performance** on more niche or fine-grained datasets like CUB — and **leave this investigation to future work**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45687521",
   "metadata": {},
   "source": [
    "## Future Impact\n",
    "\n",
    "### This Paper Sparked a Revolution\n",
    "\n",
    "It directly inspired a wave of groundbreaking models:\n",
    "\n",
    "| **Model**            | **Key Innovation**                                 |\n",
    "|----------------------|-----------------------------------------------------|\n",
    "| **DALL·E 2**         | CLIP + Diffusion                                    |\n",
    "| **GLIDE**            | Guided diffusion with text-image similarity         |\n",
    "| **Imagen**           | Large Language Model (LM) + Diffusion              |\n",
    "| **Parti**            | Autoregressive image generation                     |\n",
    "| **Stable Diffusion** | Open-source latent diffusion model                  |\n",
    "\n",
    "> *Table: Notable models inspired by Zero-Shot Text-to-Image Generation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b40844",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/thank-you.jpg\" alt=\"thank you\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b17ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
