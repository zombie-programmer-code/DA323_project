{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033b1c7c",
   "metadata": {},
   "source": [
    "# DA323 final project\n",
    "## Zero Shot Text to Image Generation\n",
    "### **Author: Saptarshi Mukherjee**\n",
    "### **Roll Number: 220150019**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a199b08",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- Motivation  \n",
    "- Historical Perspective and Connection to Multimodal Learning\n",
    "- About the Paper\n",
    "- Previous research\n",
    "- Key Innovation amd Method  \n",
    "- Datasets  \n",
    "- Significance and Results  \n",
    "- Future Impact  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73bfb8",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Text-to-image synthesis is one of the most interesting applications of deep generative models. It requires an understanding of language and the ability to create visual representations, pushing the boundaries of what AI can interpret and generate. It is an interesting multimodal application, as the ability to generate images from only text prompts is quite fascinating. I chose this topic because it blends vision and language in a unique way-with Byte-Pair encoded text tokens and discretized image tokens, leveraging an autoencoder for image compression and a transformer to model the text and image tokens as a single stream of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce7f3f",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/cgan.webp\" alt=\"CGAN Diagram\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393ff92",
   "metadata": {},
   "source": [
    "## Historical Perspective and Connection to Multimodal Learning\n",
    "\n",
    "Text-to-image generation is a hallmark of multimodal learning, requiring the fusion of natural language understanding with visual synthesis. Early efforts in this domain include:\n",
    "\n",
    "- **Conditional Variational Autoencoders (CVAE)** and **Conditional GANs (cGANs)**, which conditioned image generation on class labels or text. However, they often produced blurry or semantically mismatched images due to limited modeling capacity.\n",
    "\n",
    "- **StackGAN (2016)** introduced a two-stage generation process: a low-resolution image from the text in Stage-I and a refined high-resolution image in Stage-II. This hierarchical approach improved both structure and detail.\n",
    "\n",
    "- **AttnGAN (2018)** improved upon StackGAN by introducing **word-level attention**, allowing the generator to focus on relevant words while generating specific regions of the image. This made the outputs more coherent and better aligned with fine-grained captions.\n",
    "\n",
    "- **DM-GAN**, **ControlGAN**, and **MirrorGAN** each contributed ideas like dynamic memory modules, stronger conditioning discriminators, and textual feedback loops to reinforce the alignment between text and image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfaee7",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/controlGAN.png\" alt=\"ControlGAN Diagram\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae2ab0",
   "metadata": {},
   "source": [
    "However, all these models had one major limitation:  \n",
    "> **They relied heavily on paired training data** (i.e., captions and corresponding images).\n",
    "\n",
    "The **Zero-Shot Text-to-Image Generation** paper breaks this dependency. Instead of learning from matched pairs, it:\n",
    "- Uses a **discrete VAE** to compress image data into learnable tokens (removing the need to model pixels directly),\n",
    "- Trains a **large transformer** to model the joint distribution of text and image tokens, without any fine-tuning on specific image-caption datasets,\n",
    "- And leverages **zero-shot generalization**, enabling it to synthesize plausible images for captions it has never seen during training.\n",
    "\n",
    "This marks a major shift in the field: moving from **paired supervised learning** to **foundation models** trained with **unsupervised or weakly supervised multimodal objectives**, in line with other breakthroughs like:\n",
    "- **CLIP (Contrastive Language–Image Pretraining)** for text-image alignment without supervision,\n",
    "- **T5 and GPT-3** for general-purpose sequence modeling,\n",
    "- And later, **DALL·E**, which refined and expanded this approach using diffusion and larger-scale training.\n",
    "\n",
    "In essence, this paper represents the convergence of three trends:\n",
    "- Autoregressive transformers from NLP (like GPT),\n",
    "- Tokenized vision models via VAEs or quantization,\n",
    "- And scalable multimodal training with web-scale data.\n",
    "\n",
    "It lays the foundation for modern zero-shot generative models, moving the field closer to unified generative intelligence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823050b",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/gen1.png\" alt=\"Generated images\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066c761",
   "metadata": {},
   "source": [
    "\n",
    "Traditionally, **text-to-image synthesis** has been approached by improving modeling assumptions for training on a fixed dataset.\n",
    "\n",
    "These approaches often rely on:\n",
    "- Complex architectures  \n",
    "- Auxiliary losses  \n",
    "- Object part labels  \n",
    "- Dataset-specific design choices  \n",
    "\n",
    "Such methods can **hurt generalization**, making them less effective on unseen prompts or domains.\n",
    "\n",
    "---\n",
    "\n",
    "To counter these limitations, the authors propose a **simple and scalable approach**:\n",
    "- Use an **autoregressive transformer**  \n",
    "- Model **text and image tokens as a single stream of data**  \n",
    "\n",
    "With enough data, this method proves **competitive with task-specific models** when evaluated in a **zero-shot** setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4a1f5",
   "metadata": {},
   "source": [
    "## Generating Images from Captions with Attention\n",
    "\n",
    "In **Mansimov et al., 2015**, the authors extended the **Deep Recurrent Attention Writer (DRAW)** technique to train a model that:\n",
    "\n",
    "- Iteratively draws patches on a canvas  \n",
    "- Attends to relevant words in the text description at each step  \n",
    "\n",
    "### Key Mechanism:\n",
    "- The current hidden state for image generation is computed using:\n",
    "  - The **previous hidden state**, and  \n",
    "  - An **alignment score** between:\n",
    "    - The hidden states from the text  \n",
    "    - The previous image hidden state  \n",
    "\n",
    "- This current hidden state is then used to **generate the image at the current iteration**\n",
    "\n",
    "This was one of the **first deep learning-based approaches** for image synthesis from natural language descriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0b71c",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/variational_auto_encoder.png\" alt=\"dVAE Diagram\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2e07b",
   "metadata": {},
   "source": [
    "## StackGAN++ (Xu et al., 2018)\n",
    "\n",
    "**StackGAN-v1** uses a **2-stage Generative Adversarial Network** for text-to-image synthesis:\n",
    "\n",
    "- **Stage I**:\n",
    "  - Sketches primitive features like **color** and **shape** based on the text description.\n",
    "  - Produces **low-resolution images**.\n",
    "\n",
    "- **Stage II**:\n",
    "  - Takes Stage I output **and** the original text as input.\n",
    "  - Generates **high-resolution images** by refining the initial sketch.\n",
    "\n",
    "---\n",
    "\n",
    "An extended approach (**StackGAN++**) uses:\n",
    "- **Multiple generators and discriminators** arranged in a **tree structure**.\n",
    "- Images at **multiple scales** for the same scene are generated from **different branches** of the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bacf355",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/stackgan-v1.png\" alt=\"dVAE Diagram\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288965a",
   "metadata": {},
   "source": [
    "# About the Paper\n",
    "\n",
    "**Paper Title:**  \n",
    "*Zero-Shot Text-to-Image Generation*  \n",
    "\n",
    "**Authors:**  \n",
    "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever (OpenAI)  \n",
    "\n",
    "**Published:**  \n",
    "arXiv preprint: [https://arxiv.org/abs/2102.12092](https://arxiv.org/abs/2102.12092)  \n",
    "Date: February 2021  \n",
    "\n",
    "**Core Contribution:**  \n",
    "A scalable, autoregressive transformer-based model that treats text and image tokens as a single sequence.  \n",
    "Capable of generating realistic images from text prompts **without any task-specific fine-tuning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9d4fe",
   "metadata": {},
   "source": [
    "## Motivation for this Paper\n",
    "\n",
    "Earlier models like **GANs** and **VAEs** struggle with generalization — that is, they perform poorly on unseen data.\n",
    "\n",
    "Major issues with GANs include:\n",
    "- **Mode collapse**  \n",
    "  The GAN ends up producing data very similar to only a few types (the “modes”), which adversely affects the variety of obtained multi-modal data.\n",
    "  \n",
    "- **Nonconvergence and instability**  \n",
    "  This can arise due to:\n",
    "  - Inappropriate design of network architecture  \n",
    "  - Poor choice of objective function  \n",
    "  - Suboptimal optimization algorithms\n",
    "\n",
    "Training instability is a critical problem. For example, if the discriminator can easily distinguish between fake and real images, its gradient vanishes —\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ebdf7",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/mode_collapse.png\" alt=\"Mode collapse\" width=\"600\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdaaff0",
   "metadata": {},
   "source": [
    "## Method described in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c00ec8",
   "metadata": {},
   "source": [
    "## Key Innovation and Method\n",
    "\n",
    "The goal of this paper is to train a **transformer** that **autoregressively models both images and text as a single data stream**.\n",
    "\n",
    "However, directly using pixel values requires **too much memory**, especially for high-resolution images.  \n",
    "To overcome this, the authors **compress images into tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### Main Components of the Architecture:\n",
    "\n",
    "- **Discrete Variational Autoencoder (dVAE):**  \n",
    "  Compresses each **256×256 RGB image** into a **32×32 grid of image tokens**.\n",
    "\n",
    "- Each token can take on **8192 possible values**, i.e., from a discrete vocabulary of size 8192.\n",
    "\n",
    "- This reduces the **context length for the transformer by a factor of 192**, with **minimal degradation in visual quality** (as demonstrated in the next section).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b55c74",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/dVAE1.png\" alt=\"dVAE Diagram\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432da45",
   "metadata": {},
   "source": [
    "## The Second Stage of the Model\n",
    "\n",
    "- Up to **256 BPE-encoded text tokens** are concatenated with the **32 × 32 = 1024 image tokens**.\n",
    "- The **autoregressive transformer** is trained to model the **joint distribution over both text and image tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective Function and Interpretation:\n",
    "\n",
    "The model aims to learn the joint probability of:\n",
    "- **Text prompt** $y$\n",
    "- **Image** $x$\n",
    "- **Latent image tokens** $z$ (from the discrete VAE)\n",
    "\n",
    "This joint distribution is modeled as:\n",
    "\n",
    "**$p_{\\theta, \\psi}(x, y, z) = p_{\\theta}(x \\mid y, z) \\cdot p_{\\psi}(y, z)$**\n",
    "\n",
    "Where:\n",
    "- $p_{\\theta}(x \\mid y, z)$: Conditional image generation given text and image tokens  \n",
    "- $p_{\\psi}(y, z)$: Prior over text and image tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec223dca",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "Since we cannot directly optimize the joint probability $p_{\\theta, \\psi}(x, y)$,  we instead **maximize the Evidence Lower Bound (ELBO):**\n",
    "\n",
    "**ELBO:**\n",
    "\n",
    "$\\ln p_{\\theta, \\psi}(x, y) \\geq \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)} \\left[ \\ln p_{\\theta}(x \\mid y, z) \\right] - \\beta \\, D_{\\mathrm{KL}} \\left( q_{\\phi}(y, z \\mid x) \\,\\|\\, p_{\\psi}(y, z) \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of Terms:\n",
    "\n",
    "- $x$ — Input RGB image  \n",
    "- $y$ — Text caption  \n",
    "- $z$ — Discrete latent tokens from the dVAE  \n",
    "- $q_{\\phi}(z \\mid x)$ — Distribution over image tokens generated by the dVAE encoder  \n",
    "- $p_{\\theta}(x \\mid y, z)$ — Distribution over RGB images generated by the dVAE decoder  \n",
    "- $p_{\\psi}(y, z)$ — Prior modeled by the transformer over text and image tokens  \n",
    "- $\\beta$ — KL divergence weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab764cc",
   "metadata": {},
   "source": [
    "## Interpretation of the Objective Function\n",
    "\n",
    "Optimizing the term $\\ln p_{\\theta}(x \\mid y, z)$ helps improve the **reconstruction quality** of the dVAE.  \n",
    "This ensures that the **image token compression retains as much information** from the original image as possible.\n",
    "\n",
    "---\n",
    "\n",
    "The KL divergence term:\n",
    "\n",
    "$\n",
    "D_{\\mathrm{KL}} \\left( q_{\\phi}(y, z \\mid x) \\,\\|\\, p_{\\psi}(y, z) \\right)\n",
    "$\n",
    "\n",
    "acts as a **regularizer**, similar to its role in standard autoencoders:\n",
    "\n",
    "- It **prevents the dVAE encoder from encoding gibberish**\n",
    "- It discourages the encoder from **memorizing input images**\n",
    "- It encourages the encoder to **learn shared structure** across different images in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f00164",
   "metadata": {},
   "source": [
    "## Architecture in Brief: dVAE\n",
    "\n",
    "The **dVAE encoder and decoder** are convolutional **ResNets** (He et al., 2016), inspired by the bottleneck-style residual blocks introduced in earlier convolutional networks (LeCun et al., 1998).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Architecture Details:\n",
    "\n",
    "- Both encoder and decoder use primarily **3 × 3 convolutions**\n",
    "- **1 × 1 convolutions** are used in **skip connections** where the number of feature maps changes\n",
    "- The **first convolution of the encoder** is **7 × 7**\n",
    "- The **final encoder layer** is a **1 × 1 convolution** that produces a **32 × 32 × 8192** output (used as logits for the image token categorical distribution)\n",
    "- The **first and last layers of the decoder** are also **1 × 1 convolutions**\n",
    "\n",
    "---\n",
    "\n",
    "### Downsampling & Upsampling:\n",
    "\n",
    "- The **encoder** uses **max-pooling** to downsample feature maps *(empirically found to yield better ELBO than average pooling)*\n",
    "- The **decoder** uses **nearest-neighbor upsampling**\n",
    "\n",
    "---\n",
    "\n",
    "### Optimization:\n",
    "\n",
    "- Parameters are updated using the **AdamW optimizer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79842885",
   "metadata": {},
   "source": [
    "## Architecture in Brief: Transformer\n",
    "\n",
    "The authors present a **decoder-only transformer** designed to model the **joint distribution of text and image tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Overview:\n",
    "\n",
    "- A **12-billion parameter** transformer  \n",
    "- Processes sequences of:\n",
    "  - Up to **256 BPE-encoded text tokens**\n",
    "  - Followed by **1,024 image tokens** from the dVAE\n",
    "\n",
    "---\n",
    "\n",
    "### Attention Mechanism:\n",
    "\n",
    "- Uses **causal self-attention**, so each image token can attend to:\n",
    "  - All **preceding text tokens**\n",
    "  - All **preceding image tokens**\n",
    "\n",
    "This enables the generation of **coherent images conditioned on textual descriptions**.\n",
    "\n",
    "---\n",
    "\n",
    "### Modality-Specific Attention Masks:\n",
    "\n",
    "- **Text-to-text:** Standard **causal masks**\n",
    "- **Image-to-image:**  \n",
    "  Uses specialized masks like:\n",
    "  - **Row masks**\n",
    "  - **Column masks**\n",
    "  - **Convolutional masks**\n",
    "\n",
    "---\n",
    "\n",
    "### Key Property:\n",
    "\n",
    "This design allows the transformer to perform **zero-shot image generation** from text prompts —  **no task-specific fine-tuning** is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dad8a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To scale the model to **12 billion parameters**, the authors collected a much larger dataset of **250 million image–text pairs from the internet**, comparable in scale to **JFT-300M**.\n",
    "\n",
    "- While **MS-COCO** was not explicitly included, some validation images (not captions) were present due to **overlap with YFCC100M**, one of the sources used.\n",
    "\n",
    "---\n",
    "\n",
    "### Filtering Techniques Used to Ensure Quality:\n",
    "\n",
    "- Captions that were **too short** or **not in English** (detected using `cld3`) were removed  \n",
    "- Captions with **generic boilerplate** text (e.g., \"photographed on \\<date\\>\") were discarded  \n",
    "- Images with **extreme aspect ratios** (outside the range [1/2, 2]) were excluded  \n",
    "  - This helps avoid cropping out important visual content during training\n",
    "\n",
    "---\n",
    "\n",
    "These preprocessing steps ensured that the resulting dataset was both **high-quality** and **diverse**, supporting effective training of the large-scale transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166526c",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/image_preprocess_transformer.png\" alt=\"Preprocess steps\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef058f30",
   "metadata": {},
   "source": [
    "## Significance and Results\n",
    "\n",
    "The dataset used in this paper — **250 million image–text pairs** — is significantly larger than previously used datasets such as **MS-COCO** and **CUB-200**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "> **More data + larger model = better generalization**\n",
    "\n",
    "---\n",
    "\n",
    "### Zero-Shot Evaluation:\n",
    "\n",
    "The model was evaluated in a **zero-shot setting** against three prior state-of-the-art approaches:\n",
    "\n",
    "- **AttnGAN** (Xu et al., 2018)  \n",
    "- **DMGAN** (Zhu et al., 2019)  \n",
    "- **DF-GAN** (Tao et al., 2020)  \n",
    "  - Reports the best **Inception Score (IS)** and **Fréchet Inception Distance (FID)** on MS-COCO  \n",
    "  - Metrics based on:  \n",
    "    - Inception Score: *Salimans et al., 2016*  \n",
    "    - FID: *Heusel et al., 2017*\n",
    "\n",
    "---\n",
    "\n",
    "### Human Evaluation:\n",
    "\n",
    "A human preference study, similar to **Koh et al. (2021)**, was conducted to compare the model's performance to DF-GAN.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb60682",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/results.png\" alt=\"Results\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1dcb98",
   "metadata": {},
   "source": [
    "### Some more details about how the evaluation process was carried out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf880aef",
   "metadata": {},
   "source": [
    "The authors used 1000 captions to generate one image per model per caption. These were turned into 1000 comparison tasks on Amazon Mechanical Turk. Each task was answered by five workers, who judged which image was (1) more realistic and (2) better matched the caption. One worker's responses were disqualified due to low agreement and very fast submissions; all others were retained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757fd7a",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/human_eval.png\" alt=\"Results\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae5bb3",
   "metadata": {},
   "source": [
    "### Results on CUB, a specialized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c789a0e",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/cub_results.png\" alt=\"CUB results\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608fb492",
   "metadata": {},
   "source": [
    "## Significance and Results \n",
    "\n",
    "The model performs **significantly worse on the CUB dataset**, with nearly a **40-point gap in FID** compared to the leading prior approach.\n",
    "\n",
    "- Although there was a **12% image overlap** found in the CUB dataset, the authors observed **no significant difference in results** after removing these overlapping images.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Observation:\n",
    "\n",
    "- The **zero-shot approach** tends to struggle on **specialized distributions** like CUB.\n",
    "- This suggests that **general-purpose models may underperform** on domain-specific datasets without adaptation.\n",
    "\n",
    "---\n",
    "\n",
    "### Future Direction:\n",
    "\n",
    "- The authors identify **fine-tuning** as a **promising path for improving performance** on more niche or fine-grained datasets like CUB — and **leave this investigation to future work**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99327fa4",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "\n",
    "- **Image generation can be framed as language modeling**: One of the most important takeaways is that a transformer trained purely as a language model (over both text and image tokens) can learn to generate high-quality, semantically aligned images. This reframes image synthesis as a token prediction problem rather than pixel modeling.\n",
    "\n",
    "- **Discrete representations simplify image modeling**: Using a discrete VAE to tokenize images into a fixed vocabulary (like words) makes it possible to apply standard autoregressive transformers to vision tasks. This insight bridges the gap between NLP and computer vision in a scalable way.\n",
    "\n",
    "- **Text and image fusion doesn't need explicit alignment**: Unlike older models that explicitly fused modalities via cross-attention or feature concatenation, this approach uses a single sequence of tokens, letting the transformer learn multimodal relationships implicitly. That simplicity, enabled by scale, is both elegant and powerful.\n",
    "\n",
    "- **Zero-shot generalization is achievable**: By training on a large web-scale dataset without curated supervision, the model learns to generalize to unseen prompts. This shows that broad data coverage and architectural flexibility can outperform traditional supervised pipelines.\n",
    "\n",
    "- **Token granularity and positional encoding matter**: The use of structured attention masks (like row, column, and convolutional attention for image tokens) was a subtle but crucial trick to help the model reason spatially — a useful design takeaway for any future multimodal work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e76007",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "### What Surprised Me\n",
    "\n",
    "- **Discrete image tokens actually work**: I was initially skeptical about compressing continuous image data into discrete tokens (via dVAE), but the model still generates high-quality images. This abstraction surprisingly retains enough semantic detail for downstream generation.\n",
    "- **Unified token stream for text and image**: Instead of using separate encoders and fusion mechanisms, the transformer simply learns to predict the entire (text + image) token sequence. This simplicity in architecture, combined with scale, turned out to be extremely powerful.\n",
    "- **Zero-shot capability**: The model can generate semantically meaningful images for captions it was never explicitly trained on. This shows the strength of large-scale language-image joint modeling and is a significant step toward general-purpose generative systems.\n",
    "- **Minimal inductive bias**: There’s no explicit alignment module, segmentation supervision, or attention maps guiding which word maps to which region — and yet the model learns to do this implicitly just by modeling token sequences.\n",
    "\n",
    "### Scope for Improvement\n",
    "\n",
    "- **Visual fidelity and global consistency**: While the model captures the gist of the caption, the finer structural details (e.g., object proportions, realistic shadows) can sometimes be inconsistent, especially with unusual prompts.\n",
    "- **Representation bottleneck of dVAE**: Although tokenizing images makes modeling efficient, the quantization process can limit sharpness or introduce artifacts. Later methods like VQ-GAN and diffusion address this more effectively.\n",
    "- **Slow inference and sampling**: Since the transformer predicts 1024 tokens autoregressively, image generation is relatively slow compared to feed-forward models. This limits real-time applications.\n",
    "- **No explicit control**: There's limited fine-grained control over spatial arrangements, styles, or specific visual attributes in the output. Later models try to address this with layout conditioning or style prompts.\n",
    "- **Evaluation gap**: While the model produces visually plausible images, evaluating *semantic alignment* with captions remains tricky. Human judgment is still essential, as automated metrics like FID or CLIP score don’t always reflect quality or relevance. Reranking the sample using the contrastive model can be slow and it dependsheavily on how good the contrastive model is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f8eae",
   "metadata": {},
   "source": [
    "## Future Impact\n",
    "\n",
    "### This Paper Sparked a Revolution\n",
    "\n",
    "It directly inspired a wave of groundbreaking models:\n",
    "\n",
    "| **Model**            | **Key Innovation**                                 |\n",
    "|----------------------|-----------------------------------------------------|\n",
    "| **DALL·E 2**         | CLIP + Diffusion                                    |\n",
    "| **GLIDE**            | Guided diffusion with text-image similarity         |\n",
    "| **Imagen**           | Large Language Model (LM) + Diffusion              |\n",
    "| **Parti**            | Autoregressive image generation                     |\n",
    "| **Stable Diffusion** | Open-source latent diffusion model                  |\n",
    "\n",
    "> *Table: Notable models inspired by Zero-Shot Text-to-Image Generation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc8bc5",
   "metadata": {},
   "source": [
    "## References\n",
    "- Xu et al. (2018), *AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks*, [arXiv:1711.10485](https://arxiv.org/abs/1711.10485)\n",
    "- Zhang et al. (2018), *StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks*, [arXiv:1710.10916](https://arxiv.org/abs/1710.10916)\n",
    "- Ramesh et al. (2021), *Zero Shot Text to Image Generation* [arXiv:2102.12092](https://arxiv.org/abs/2102.12092)  \n",
    "- Code repository: [GitHub - zombie-programmer-code/DA323_project](https://github.com/zombie-programmer-code/DA323_project)\n",
    "- Further reading: [Distill.pub](https://distill.pub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f79b6a",
   "metadata": {},
   "source": [
    "### Link to coding demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ca7ac",
   "metadata": {},
   "source": [
    "[Click here to view the code on Kaggle](https://www.kaggle.com/code/saptarshim596/da323-project-demo?scriptVersionId=238133612)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb056337",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zombie-programmer-code/DA323_project/main/project_images/thank-you.jpg\" alt=\"thank you\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e8215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
